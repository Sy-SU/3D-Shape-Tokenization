# models/shape_tokenizer.py

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils import FourierPositionalEncoding3D, PointFeatureProjector, RMSNorm

# ========== Cross Attention Block for Shape Tokenizer ==========


class CrossAttentionBlock(nn.Module):
    """
    å®ç° shape tokenizer ä¸­çš„ Cross Attention æ¨¡å—ã€‚

    è¾“å…¥:
        query_tokens: Tensor[B, k, d_f]ï¼Œå¯å­¦ä¹ çš„åˆå§‹ token
        point_features: Tensor[B, n, d_f]ï¼Œä½ç½®ç¼–ç å¹¶æŠ•å½±åçš„ç‚¹äº‘ç‰¹å¾
    è¾“å‡º:
        attended_tokens: Tensor[B, k, d_f]ï¼Œèšåˆäº†ç‚¹äº‘ä¿¡æ¯çš„ token è¡¨ç¤º

    æ¶æ„:
        - LayerNorm (query / key-value åˆ†å¼€)
        - Linear_q / Linear_k / Linear_v
        - RMSNorm (query / key åˆ†å¼€)
        - Scaled Dot-Product Attention
        - Linear_o æŠ•å½±
        - Residual + FeedForward: LayerNorm â†’ Linear â†’ GELU â†’ Linear â†’ Residual
    """
    def __init__(self, d_f, n_heads=8):
        super().__init__()
        self.d_f = d_f
        self.n_heads = n_heads
        self.head_dim = d_f // n_heads

        assert d_f % n_heads == 0, "d_f å¿…é¡»å¯ä»¥è¢« n_heads æ•´é™¤"

        # LayerNorm
        self.layernorm_q = nn.LayerNorm(d_f)
        self.layernorm_kv = nn.LayerNorm(d_f)

        # Linear projections
        self.linear_q = nn.Linear(d_f, d_f)
        self.linear_k = nn.Linear(d_f, d_f)
        self.linear_v = nn.Linear(d_f, d_f)

        # RMSNorm (æ ‡å‡†å®ç°)
        self.rmsnorm_q = RMSNorm(d_f)
        self.rmsnorm_k = RMSNorm(d_f)

        # Output projection
        self.linear_o = nn.Linear(d_f, d_f)

        # ===== MLP + Residual Block=====
        self.mlp_norm = nn.LayerNorm(d_f)  # MLP å‰çš„ LayerNorm
        self.mlp = nn.Sequential(          # å‰é¦ˆç½‘ç»œï¼šLinear â†’ GELU â†’ Linear
            nn.Linear(d_f, d_f * 4),
            nn.GELU(),
            nn.Linear(d_f * 4, d_f)
        )

    def forward(self, query_tokens, point_features):
        """
        å‚æ•°:
            query_tokens: Tensor[B, k, d_f]
            point_features: Tensor[B, n, d_f]
        è¿”å›:
            Tensor[B, k, d_f]
        """
        B, k_token, d_f = query_tokens.shape
        _, n_point, _ = point_features.shape

        # LayerNorm
        q_input = self.layernorm_q(query_tokens)
        kv_input = self.layernorm_kv(point_features)

        # Linear projections
        q_proj = self.linear_q(q_input)
        k_proj = self.linear_k(kv_input)
        v_proj = self.linear_v(kv_input)

        # RMSNorm
        q_proj = self.rmsnorm_q(q_proj)
        k_proj = self.rmsnorm_k(k_proj)

        # Reshape for multi-head attention
        q_proj = q_proj.view(B, k_token, self.n_heads, self.head_dim).transpose(1, 2)  # [B, h, k, d_h]
        k_proj = k_proj.view(B, n_point, self.n_heads, self.head_dim).transpose(1, 2)  # [B, h, n, d_h]
        v_proj = v_proj.view(B, n_point, self.n_heads, self.head_dim).transpose(1, 2)  # [B, h, n, d_h]

        # Scaled dot-product attention
        scores = torch.matmul(q_proj, k_proj.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, h, k, n]
        attn = torch.softmax(scores, dim=-1)  # [B, h, k, n]
        out = torch.matmul(attn, v_proj)  # [B, h, k, d_h]

        # åˆå¹¶å¤šå¤´è¾“å‡º
        out = out.transpose(1, 2).contiguous().view(B, k_token, d_f)  # [B, k, d_f]

        # Residual 1: attention + query_tokens
        out = self.linear_o(out) + query_tokens

        # Residual 2: MLP(layernorm(attn + query)) + (attn + query)
        mlp_out = self.mlp(self.mlp_norm(out))
        out = out + mlp_out

        return out

# ========== Self Attention Block for Shape Tokenizer ==========


class SelfAttentionBlock(nn.Module):
    """
    å®ç° shape tokenizer ä¸­çš„ Self Attention æ¨¡å—ï¼ˆé‡å¤ 2 æ¬¡ï¼‰ã€‚

    è¾“å…¥:
        tokens: Tensor[B, k, d_f]ï¼Œè·¨ token çš„ä¸Šä¸‹æ–‡å»ºæ¨¡
    è¾“å‡º:
        tokens: Tensor[B, k, d_f]ï¼Œèåˆä¸Šä¸‹æ–‡ä¿¡æ¯çš„ token è¡¨ç¤º

    æ¶æ„:
        - LayerNorm
        - Linear_q / Linear_k / Linear_vï¼ˆæ¥è‡ªç›¸åŒè¾“å…¥ï¼‰
        - RMSNorm_q / RMSNorm_kï¼ˆåˆ†å¼€ï¼‰
        - å¤šå¤´æ³¨æ„åŠ›ï¼ˆquery, key, value éƒ½æ˜¯ tokensï¼‰
        - Linear_o æŠ•å½±
        - Residual + FeedForward: LayerNorm â†’ Linear â†’ GELU â†’ Linear â†’ Residual
    """
    def __init__(self, d_f, d, n_heads=8):
        super().__init__()
        self.d_f = d_f
        self.d = d
        self.n_heads = n_heads
        self.head_dim = d_f // n_heads

        assert d_f % n_heads == 0, "d_f å¿…é¡»å¯ä»¥è¢« n_heads æ•´é™¤"

        # LayerNorm
        self.layernorm = nn.LayerNorm(d_f)

        # Linear projections
        self.linear_q = nn.Linear(d_f, d_f)
        self.linear_k = nn.Linear(d_f, d_f)
        self.linear_v = nn.Linear(d_f, d_f)

        # RMSNorm
        self.rmsnorm_q = RMSNorm(d_f)
        self.rmsnorm_k = RMSNorm(d_f)

        # Output projection
        self.linear_o = nn.Linear(d_f, d_f)

        # MLP + Residual Block
        self.mlp_norm = nn.LayerNorm(d_f)
        self.mlp = nn.Sequential(
            nn.Linear(d_f, d_f * 4),
            nn.GELU(),
            nn.Linear(d_f * 4, d_f)
        )

    def forward(self, tokens):
        """
        å‚æ•°:
            tokens: Tensor[B, k, d_f]
        è¿”å›:
            Tensor[B, k, d_f]
        """
        B, k_token, d_f = tokens.shape

        # LayerNorm
        x_norm = self.layernorm(tokens)

        # Linear projections
        q_proj = self.linear_q(x_norm)
        k_proj = self.linear_k(x_norm)
        v_proj = self.linear_v(x_norm)

        # RMSNorm
        q_proj = self.rmsnorm_q(q_proj)
        k_proj = self.rmsnorm_k(k_proj)

        # Multi-head attention reshape
        q_proj = q_proj.view(B, k_token, self.n_heads, self.head_dim).transpose(1, 2)  # [B, h, k, d_h]
        k_proj = k_proj.view(B, k_token, self.n_heads, self.head_dim).transpose(1, 2)  # [B, h, k, d_h]
        v_proj = v_proj.view(B, k_token, self.n_heads, self.head_dim).transpose(1, 2)  # [B, h, k, d_h]

        # Attention
        scores = torch.matmul(q_proj, k_proj.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, h, k, k]
        attn = torch.softmax(scores, dim=-1)
        out = torch.matmul(attn, v_proj)  # [B, h, k, d_h]

        # åˆå¹¶å¤šå¤´è¾“å‡º
        out = out.transpose(1, 2).contiguous().view(B, k_token, d_f)  # [k, d_f]

        # Residual 1
        out = self.linear_o(out) + tokens

        # Residual 2
        mlp_out = self.mlp(self.mlp_norm(out))
        out = out + mlp_out

        return out

# ========== Token Block ==========


class TokenBlock(nn.Module):
    """
    å°è£…ä¸€ä¸ª block: CrossAttention + SelfAttention Ã—2
    æ¯æ¬¡è¿­ä»£ä»ç‚¹äº‘æå–ä¿¡æ¯å¹¶èåˆä¸Šä¸‹æ–‡
    """
    def __init__(self, d_f, d, n_heads):
        super().__init__()
        self.cross_attn = CrossAttentionBlock(d_f, n_heads)
        self.self_attn1 = SelfAttentionBlock(d_f, d, n_heads)
        self.self_attn2 = SelfAttentionBlock(d_f, d, n_heads)

    def forward(self, tokens, point_features):
        tokens = self.cross_attn(tokens, point_features)
        tokens = self.self_attn1(tokens)
        tokens = self.self_attn2(tokens)
        return tokens

# ========== Shape Tokenizer ==========


class ShapeTokenizer(nn.Module):
    """
    å®ç° Shape Tokenizerã€‚

    è¾“å…¥:
        points: Tensor[n, 3]ï¼ŒåŸå§‹ç‚¹äº‘åæ ‡
    è¾“å‡º:
        shape_tokens: Tensor[k, d_f]ï¼Œç”¨äºç”Ÿæˆçš„å½¢çŠ¶ token è¡¨è¾¾

    æ¨¡å—ç»“æ„:
        - FourierPositionalEncoding3D
        - PointFeatureProjector
        - åˆå§‹åŒ– learnable tokens
        - 6 Ã— (CrossAttentionBlock + SelfAttentionBlock Ã— 2)
    """
    def __init__(self, num_tokens: int = 32, d_in: int = 3, d_f: int = 512, d: int = 64, n_heads: int = 8, num_frequencies: int = 16, num_blocks: int = 16):
        super().__init__()
        self.k = num_tokens
        self.d_f = d_f
        self.d = d
        self.output_proj = nn.Linear(d_f, d)

        # Fourier ç¼–ç å™¨ + æŠ•å½±æ¨¡å—
        self.pos_encoder = FourierPositionalEncoding3D(num_frequencies=num_frequencies, include_input=True)
        pe_dim = 3 + 3 * 2 * num_frequencies  # è¾“å…¥ä¸º3ç»´ç‚¹ï¼Œæ¯ç»´äº§ç”Ÿ 2 Ã— freq ä¸ªæ–°ç‰¹å¾
        self.projector = PointFeatureProjector(pe_dim, d_f)

        # åˆå§‹åŒ–å¯å­¦ä¹  token å‘é‡ [k, d_f]
        self.token_embed = nn.Parameter(torch.randn(num_tokens, d_f))

        # 6 ä¸ªé‡å¤ block
        self.blocks = nn.ModuleList([TokenBlock(d_f, d, n_heads) for _ in range(num_blocks)])

    def forward(self, x):
        """
        å‚æ•°:
            x: Tensor[B, n, 3]ï¼ŒåŸå§‹ç‚¹äº‘è¾“å…¥
        è¿”å›:
            shape_tokens: Tensor[B, k, d_f]
        """
        B, N, _ = x.shape

        # Step 1: Fourier ç¼–ç  â†’ [n, pe_dim]
        x_encoded = self.pos_encoder(x.view(-1, 3))  # [B*N, pe_dim]

        # Step 2: æŠ•å½± â†’ [n, d_f]
        x_projected = self.projector(x_encoded).view(B, N, self.d_f)  # [B, N, d_f]

        # Step 3: 6 Ã— (Cross Attention + Self Attention Ã— 2)
        tokens = self.token_embed.unsqueeze(0).expand(B, -1, -1)   # [B, k, d_f]
        for block in self.blocks:
            tokens = block(tokens, x_projected)

        token_out = self.output_proj(tokens)

        return token_out

# ========== Unit Test ==========


if __name__ == '__main__':
    # ========== Unit Test of Cross Attention Block ==========
    torch.manual_seed(42)

    B = 4
    d_f = 128
    k = 32    # number of learned tokens
    n = 2048  # number of input point features

    query = torch.randn(B, k, d_f)
    points = torch.randn(B, n, d_f)

    print("âœ… Input shapes:")
    print("  Query shape:", query.shape)
    print("  Point feature shape:", points.shape)

    cross_attn = CrossAttentionBlock(d_f=d_f, n_heads=8)
    out = cross_attn(query, points)

    # æ£€æŸ¥è¾“å‡ºç»´åº¦æ­£ç¡®
    assert out.shape == (B, k, d_f), f"Output shape mismatch: got {out.shape}, expected ({k}, {d_f})"

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰é™å€¼ï¼ˆæ—  inf / nanï¼‰
    assert torch.isfinite(out).all(), "Output contains NaN or Inf!"

    print("âœ… Output shape:", out.shape)
    print("âœ… Output sample:", out[0][:6])
    print("ğŸ‰ CrossAttentionBlock unit test passed.")
    # ========== End Unit Test of Cross Attention Block ==========

    # ========== Unit Test of Self Attention Block ==========
    tokens = torch.randn(B, k, d_f)
    self_attn = SelfAttentionBlock(d_f=d_f, n_heads=8)
    out2 = self_attn(tokens)

    # æ£€æŸ¥è¾“å‡ºç»´åº¦æ­£ç¡®
    assert out2.shape == (B, k, d_f), f"SelfAttention output shape mismatch: got {out2.shape}, expected ({k}, {d_f})"

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰é™å€¼
    assert torch.isfinite(out2).all(), "SelfAttention output contains NaN or Inf!"

    print("âœ… SelfAttention output shape:", out2.shape)
    print("âœ… SelfAttention output sample:", out2[0][:6])
    print("ğŸ‰ SelfAttentionBlock unit test passed.")
    # ========== End Unit Test of Self Attention Block ==========

    # ========== Unit Test of Shape Tokenizer ==========
    dummy_points = torch.randn(B, n, 3)  # åŸå§‹ç‚¹äº‘è¾“å…¥ [n, 3]
    tokenizer = ShapeTokenizer(num_tokens=k, d_in=3, d_f=d_f, n_heads=8, num_frequencies=16, num_blocks=6)
    shape_tokens = tokenizer(dummy_points)

    # æ£€æŸ¥è¾“å‡ºç»´åº¦
    assert shape_tokens.shape == (B, k, d_f), f"ShapeTokenizer output shape mismatch: got {shape_tokens.shape}, expected ({k}, {d_f})"

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰é™å€¼
    assert torch.isfinite(shape_tokens).all(), "ShapeTokenizer output contains NaN or Inf!"

    print("âœ… ShapeTokenizer output shape:", shape_tokens.shape)
    print("âœ… ShapeTokenizer output sample:", shape_tokens[0][:6])
    print("ğŸ‰ ShapeTokenizer unit test passed.")
    # ========== End Unit Test of Shape Tokenizer ==========
