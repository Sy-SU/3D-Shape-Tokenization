# tools/eval_reconstruct.py

"""
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„ ShapeTokenizer ä¸ VelocityEstimator æ¨¡å‹åœ¨ ShapeNet æµ‹è¯•é›†ä¸Šçš„é‡å»ºæ€§èƒ½ã€‚

è¯„ä¼°æŒ‡æ ‡ï¼š
- Chamfer Distanceï¼ˆ2048 pointsï¼‰

æ‰§è¡Œæ­¥éª¤ï¼š
1. åŠ è½½æµ‹è¯•é›†ç‚¹äº‘ï¼ˆ2048 pointsï¼‰
2. ä½¿ç”¨è®­ç»ƒå¥½çš„ tokenizer ç”Ÿæˆ shape token
3. ä½¿ç”¨ velocity estimator ä»å™ªå£°è§£ç ç”Ÿæˆç‚¹äº‘ï¼ˆHeun è§£ ODEï¼‰
4. ä¸åŸå§‹ç‚¹äº‘è®¡ç®— Chamfer Distance
5. æŠ¥å‘Šå¹³å‡æŒ‡æ ‡
6. å°†ç”Ÿæˆçš„ç‚¹äº‘ä¿å­˜åˆ° outs/reconstruct/<æ—¶é—´æˆ³>/
"""

import os
import sys
import time
import torch
import torch.nn as nn
from tqdm import tqdm
import numpy as np
import argparse
from torch.utils.data import DataLoader, SubsetRandomSampler
from collections import defaultdict
import math
import random


# æ·»åŠ  utils æ¨¡å—æ‰€åœ¨è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from models.shape_tokenizer import ShapeTokenizer
from models.velocity_estimator import VelocityEstimator
from datasets.dataloader import get_dataloader

# ========== è®¾ç½®å…¨å±€ä¿å­˜ç›®å½•æ—¶é—´æˆ³ ==========
TIMESTAMP = time.strftime("%Y%m%d_%H%M%S")


# ========== åŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡ ==========
def load_models(device, num_tokens, d, d_f,
                tokenizer_ckpt='checkpoints/best_tokenizer.pt',
                estimator_ckpt='checkpoints/best_estimator.pt'):
    """
    è¯´æ˜ï¼š
        ä¸ºäº†ä¸è®­ç»ƒæ—¶çš„ç»“æ„ä¿æŒä¸€è‡´ï¼Œè¿™é‡Œæ˜¾å¼æ¥æ”¶ num_tokens/d/d_f ä¸‰ä¸ªç»“æ„è¶…å‚ã€‚
        ï¼ˆä½ ä¹‹å‰çš„è„šæœ¬ç‰ˆæœ¬å°±æ˜¯è¿™ä¹ˆåšçš„ï¼Œè¿™æ ·èƒ½é¿å…ç»´åº¦ä¸ä¸€è‡´å¯¼è‡´çš„ load_state_dict å¤±è´¥ï¼‰
    """
    tokenizer = ShapeTokenizer(
        num_tokens=num_tokens,
        d_in=3,
        d_f=d_f,
        d=d,
        n_heads=8,
        num_frequencies=16,
        num_blocks=6
    ).to(device)
    tokenizer.load_state_dict(torch.load(tokenizer_ckpt, map_location=device))
    tokenizer.eval()

    estimator = VelocityEstimator(
        d=d,
        d_f=d_f,
        num_frequencies=16,
        n_blocks=3
    ).to(device)
    estimator.load_state_dict(torch.load(estimator_ckpt, map_location=device))
    estimator.eval()

    return tokenizer, estimator


# ========== é‡å»ºå‡½æ•°ï¼ˆHeun ODEï¼‰ ==========
def decode_from_token(estimator, tokens, num_points=2048, steps=1000):
    """
    ä½¿ç”¨ Heun æ–¹æ³•ä» shape token ä¸­é‡å»ºç‚¹äº‘ã€‚
    å‚æ•°:
        estimator: VelocityEstimator
        tokens: Tensor[B, K, D]ï¼ˆæ­¤å¤„ D = dï¼Œæ³¨æ„ä¼°è®¡å™¨å†…éƒ¨é€šé“æ˜¯ d_fï¼Œå¤–éƒ¨ token ç»´åº¦æ˜¯ dï¼‰
        num_points: int, é‡å»ºç‚¹æ•°
        steps: int, ODE ç§¯åˆ†æ­¥æ•°
    è¿”å›:
        Tensor[B, num_points, 3]ï¼Œé‡å»ºç‚¹äº‘
    """
    device = tokens.device
    B, K, D = tokens.shape

    h = 1.0 / steps  # æ—¶é—´æ­¥é•¿

    # æ‰©å±• shape token s ä¸º [B, num_points, K, D] â†’ [B * num_points, K, D]
    s = tokens.unsqueeze(1).expand(-1, num_points, -1, -1).reshape(B * num_points, K, D)

    # åˆå§‹åŒ–é‡å»ºç‚¹ x: [B, num_points, 3]ï¼ŒèŒƒå›´ [-1, 1]
    x_raw = (torch.rand(B, num_points, 3, device=device) * 2 - 1).view(B * num_points, 3)
    t = torch.zeros(B * num_points, device=device)  # åˆå§‹æ—¶é—´ t=0

    for _ in range(steps):
        v1 = estimator(x_raw, s, t)              # [B*num_points, 3]
        x_temp = x_raw + h * v1                  # Euler ç¬¬ä¸€æ­¥
        t_b = t + h
        v2 = estimator(x_temp, s, t_b)           # Heun ç¬¬äºŒæ­¥

        x_raw = x_raw + 0.5 * h * (v1 + v2)
        t = t_b

    recon_points = x_raw.view(B, num_points, 3)
    return recon_points.detach().cpu()


# ========== Chamfer Distanceï¼ˆæ”¯æŒ batchï¼‰ ==========
def chamfer_distance_batch(x: torch.Tensor, y: torch.Tensor) -> float:
    """
    è®¡ç®—å¯¹ç§° Chamfer è·ç¦»ï¼ˆæ”¯æŒ batchï¼‰
    å‚æ•°:
        x: Tensor[B, M, 3]
        y: Tensor[B, N, 3]
    è¿”å›:
        float: å¹³å‡ chamfer distanceï¼ˆå…ˆå¯¹æ¯ä¸ª batch æ±‚ï¼Œå†å¯¹ batch æ±‚å‡å€¼ï¼‰
    å¤‡æ³¨ï¼š
        é‡‡ç”¨ä¸ PointFlow/LION ç›¸åŒçš„â€œå¹³æ–¹æ¬§æ°è·ç¦» + åŒå‘æœ€è¿‘ç‚¹å¹³å‡â€çš„å®šä¹‰ï¼ˆå‚è§ä½ çš„å‚è€ƒè¡¨è¿°ï¼‰ã€‚ 
    """
    # [B, M, N]
    d = torch.cdist(x, y, p=2) ** 2
    cd_xy = d.min(dim=2).values.mean(dim=1)   # x->y, æŒ‰ M å¹³å‡
    cd_yx = d.min(dim=1).values.mean(dim=1)   # y->x, æŒ‰ N å¹³å‡
    cd = (cd_xy + cd_yx).mean()
    return cd.item()


# ========== è¯„ä¼°å‡½æ•° ==========
def evaluate(tokenizer, estimator, dataloader, device, save_dir, ode_steps=1000):
    os.makedirs(save_dir, exist_ok=True)

    num_points = dataloader.dataset.num_points
    print(f"Evaluating on {len(dataloader)} batches, each with {num_points} points...")

    cds = []
    running_idx = 0  # ç”¨äºç»™ä¿å­˜æ–‡ä»¶ç¼–å·ï¼ˆç¡®ä¿ä¸è¢« batch ç»´åº¦æ··æ·†ï¼‰

    for batch in tqdm(dataloader, desc="Evaluating"):
        # batch['points'] ï¼š[B, N, 3]ï¼Œç‚¹äº‘æ•°æ®
        gt = batch['points'].to(device)
        tokens = tokenizer(gt)  # [B, K, d]

        pred = decode_from_token(estimator, tokens, num_points=num_points, steps=ode_steps)

        # è®¡ç®— Chamfer Distanceï¼ˆæŒ‰ batchï¼‰
        cd = chamfer_distance_batch(pred, gt.cpu())
        cds.append(cd)

        # é€æ ·æœ¬ä¿å­˜
        B = pred.shape[0]
        for b in range(B):
            np.save(os.path.join(save_dir, f"{running_idx:05d}_recon.npy"), pred[b].numpy())
            np.save(os.path.join(save_dir, f"{running_idx:05d}_gt.npy"), gt[b].detach().cpu().numpy())
            running_idx += 1

    avg_cd = sum(cds) / len(cds)
    print(f"âœ… Chamfer Distance over {running_idx} samples: {avg_cd:.6f}")


# ========== ä¸»å…¥å£ ==========
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è¯„ä¼° ShapeTokenizer ä¸ VelocityEstimator çš„é‡å»ºæ€§èƒ½")
    parser.add_argument('--data_root', type=str, default='/root/autodl-fs/demo')
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--num_points', type=int, default=2048)
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--save_dir', type=str, default=None, help='ä¿å­˜è¾“å‡ºç›®å½•ï¼Œè‹¥ä¸ºç©ºåˆ™ä½¿ç”¨å½“å‰æ—¶é—´æˆ³')

    # ===== æ–°å¢ï¼šåœ¨æµ‹è¯•é›†ä¸­éšæœºæŠ½å–è‹¥å¹²æ ·æœ¬ =====
    parser.add_argument('--num_samples', type=int, default=64, help='åœ¨æµ‹è¯•é›†å†…éšæœºæŠ½å–çš„æ ·æœ¬æ•°ï¼›<=0 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºæŠ½æ ·çš„éšæœºç§å­')

    # ===== ç»“æ„/æƒé‡ & ODE æ­¥æ•°å‚æ•°ï¼ˆä¿æŒä¸è®­ç»ƒä¾§ä¸€è‡´ï¼‰=====
    parser.add_argument('--num_tokens', type=int, default=32)
    parser.add_argument('--d_f', type=int, default=512)  # hidden ç»´åº¦ï¼ˆTokenizer/Estimator å†…éƒ¨é€šé“ï¼‰
    parser.add_argument('--d', type=int, default=64)     # shape token ç»´åº¦ï¼ˆTokenizer è¾“å‡º & Estimator è¾“å…¥çš„ token ç»´ï¼‰
    parser.add_argument('--ode_steps', type=int, default=1000, help='Heun ODE é‡‡æ ·æ­¥æ•°')

    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer, estimator = load_models(
        device,
        num_tokens=args.num_tokens,
        d=args.d,
        d_f=args.d_f
    )

    # ä¿å­˜ç›®å½•
    if args.save_dir is None:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join("outs/reconstruct", timestamp)
    else:
        save_dir = os.path.join("outs/reconstruct", args.save_dir)

    # å…ˆåˆ›å»ºä¸€ä¸ªç”¨äºæ‹¿åˆ° dataset çš„ DataLoader
    base_loader = get_dataloader(
        root=args.data_root,
        split='test',
        batch_size=args.batch_size,
        num_points=args.num_points,
        shuffle=False,
        num_workers=args.num_workers
    )

    dataset = base_loader.dataset
    total = len(dataset)

    # ===== å‡åŒ€æŒ‰ç±»é‡‡æ · =====
    target_total = args.num_samples if (args.num_samples is not None and args.num_samples > 0) else 0
    if target_total and target_total < total:
        # 1) æŒ‰ label_name åˆ†æ¡¶
        label_to_indices = defaultdict(list)
        for idx in range(total):
            item = dataset[idx]
            # å…¼å®¹ï¼šitem å¯èƒ½æ˜¯ dict
            label = item['label_name'] if isinstance(item, dict) else getattr(item, 'label_name', None)
            if label is None:
                raise KeyError(f"Sample {idx} has no 'label_name' field.")
            label_to_indices[label].append(idx)

        num_classes = len(label_to_indices)
        rng = random.Random(args.seed)

        # 2) æ¯ç±»åˆ†é…é…é¢ï¼ˆä¸Šå–æ•´ï¼‰ï¼Œä¼˜å…ˆå‡åŒ€è¦†ç›–å„ç±»
        per_class_quota = max(1, math.ceil(target_total / num_classes))

        selected = []
        leftovers = []

        for label, idxs in label_to_indices.items():
            rng.shuffle(idxs)  # ä»¥ seed æ‰“ä¹±
            take = min(per_class_quota, len(idxs))
            selected.extend(idxs[:take])
            # è®°å½•è¯¥ç±»å‰©ä½™å¤‡ç”¨
            leftovers.extend(idxs[take:])

        # 3) å¦‚æœè¶…è¿‡ç›®æ ‡æ€»æ•°ï¼Œæˆªæ–­ï¼›å¦‚æœä¸è¶³åˆ™ä»å‰©ä½™é‡Œè¡¥é½
        if len(selected) > target_total:
            selected = selected[:target_total]
        elif len(selected) < target_total:
            rng.shuffle(leftovers)
            need = target_total - len(selected)
            selected.extend(leftovers[:need])

        sampler = SubsetRandomSampler(selected)
        dataloader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            sampler=sampler,
            num_workers=args.num_workers,
            drop_last=False
        )

        # æ‰“å°ä¸€ä¸ªç®€çŸ­åˆ†å¸ƒç»Ÿè®¡
        dist = defaultdict(int)
        for i in selected:
            lab = dataset[i]['label_name'] if isinstance(dataset[i], dict) else getattr(dataset[i], 'label_name', 'UNK')
            dist[lab] += 1
        dist_str = ", ".join([f"{k}:{v}" for k, v in list(dist.items())[:10]])
        print(f"ğŸ” Class-balanced subset: {len(selected)}/{total} samples "
              f"(classes={num_classes}, quotaâ‰ˆ{per_class_quota}).")
        print(f"   Sampled per-class (first 10): {dist_str}")
    else:
        dataloader = base_loader
        print(f"ğŸ” Using the full test set: {total} samples.")


    with torch.no_grad():
        evaluate(tokenizer, estimator, dataloader, device, save_dir, ode_steps=args.ode_steps)
