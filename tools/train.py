# tools/train.py

"""
è”åˆè®­ç»ƒ ShapeTokenizer ä¸ VelocityEstimator çš„ä¸»è„šæœ¬ã€‚

åŠŸèƒ½ï¼š
- ä» ShapeNet åŠ è½½ç‚¹äº‘æ•°æ®
- ä½¿ç”¨ ShapeTokenizer æå– latent tokens
- ä½¿ç”¨ VelocityEstimator æ‹Ÿåˆå™ªå£°ç‚¹åˆ°ç›®æ ‡ç‚¹ä¹‹é—´çš„é€Ÿåº¦åœº
- è®­ç»ƒç›®æ ‡åŒ…æ‹¬ï¼šFlow Matching Loss + KL æ­£åˆ™é¡¹
- æ”¯æŒè®­ç»ƒé›†ä¸éªŒè¯é›†è¯„ä¼°
- æ·»åŠ æ—©åœä¸æœ€ä½³æ¨¡å‹ä¿å­˜é€»è¾‘
- å°†è®­ç»ƒæ—¥å¿—å†™å…¥ logs/ ç›®å½•

"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
from tqdm import tqdm
import json
from datetime import datetime

# æ·»åŠ  utils æ¨¡å—æ‰€åœ¨è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from datasets.dataloader import get_dataloader
from models.shape_tokenizer import ShapeTokenizer
from models.velocity_estimator import VelocityEstimator

# ========== Transformer LR Scheduler ==========
def get_transformer_lr_schedule(warmup_steps, d_model):
    def lr_lambda(step):
        step += 1
        return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)
    return lr_lambda


# ========== å•è½®è®­ç»ƒ ==========
def run_one_epoch(tokenizer: ShapeTokenizer,
                  estimator: VelocityEstimator,
                  dataloader,
                  optimizer: optim.Optimizer = None,
                  scheduler: LambdaLR = None,
                  device: torch.device = None,
                  kl_weight: float = 1e-4,
                  mode: str = "train"):
    is_train = (mode == "train")
    tokenizer.train(is_train)
    estimator.train(is_train)

    total_flow_loss, total_kl_loss, num_batches = 0.0, 0.0, 0

    # éå†æ¯ä¸ª batchï¼ˆç‚¹äº‘é›†åˆï¼‰è¿›è¡Œè®­ç»ƒæˆ–éªŒè¯
    for batch in tqdm(dataloader, desc=f"{mode.capitalize()}", leave=False):
        # Step 1: æå–ç‚¹äº‘å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ (Tensor[B, N, 3])
        pointclouds = batch['points'].to(device)
        B, N, _ = pointclouds.shape

        # Step 2: è·å–æ¯ä¸ªæ ·æœ¬çš„ latent shape token (Tensor[B, K, D])
        shape_tokens = tokenizer(pointclouds)

        # Step 3: æ„é€ æ¯ä¸ªæ ·æœ¬çš„ (x0, x1, t) å¹¶è®¡ç®—ä¸­é—´ç‚¹ x_t
        # x0: å™ªå£°ç‚¹ âˆˆ Uniform([-1, 1]^3), shape: [B, 3]
        x0 = torch.rand(B, 3, device=device) * 2 - 1
        idx = torch.randint(0, N, size=(B,), device=device)
        x1 = pointclouds[torch.arange(B), idx, :]
        t = torch.rand(B, device=device)
        x_t = (1 - t.unsqueeze(1)) * x0 + t.unsqueeze(1) * x1

        # Step 4: ä½¿ç”¨ velocity estimator é¢„æµ‹ v_theta(x_t; s, t)
        v_pred = estimator(x_t, shape_tokens, t)
        # Step 5: è®¡ç®— Flow Matching Loss
        target_v = x1 - x0
        loss_flow = nn.functional.mse_loss(v_pred, target_v)

        # ä¸ºæ­¤æˆ‘ä»¬éœ€è¦å†æ¬¡é‡‡æ ·ä¸€ç»„å­ç‚¹äº‘ Z
        # Step 6: ä¸€è‡´æ€§ KL æ•£åº¦é¡¹ KL(q(s|Y) || q(s|Z))
        num_subpoints = min(1024, N)
        idx_z = torch.randint(0, N, (B, num_subpoints), device=device)  # [B, 1024]
        subpoints_z = torch.gather(
            pointclouds, 1,
            idx_z.unsqueeze(-1).expand(-1, -1, 3)
        )  # [B, 1024, 3]
        shape_tokens_z = tokenizer(subpoints_z)  # [B, K, D]

        mu_y = shape_tokens.mean(dim=1)         # [B, D]
        mu_z = shape_tokens_z.mean(dim=1)       # [B, D]
        sigma = 1e-3
        loss_kl = (1.0 / (2 * sigma ** 2)) * ((mu_y - mu_z) ** 2).sum(dim=1).mean()

        # æ€»æŸå¤± = Flow Matching + KL
        loss = loss_flow + kl_weight * loss_kl

        # Step 7: åå‘ä¼ æ’­ä¸ä¼˜åŒ–ï¼Œä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹æ‰§è¡Œ
        if is_train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if scheduler is not None:
                scheduler.step()

        total_flow_loss += loss_flow.item()
        total_kl_loss += loss_kl.item()
        num_batches += 1

    # è¿”å›å¹³å‡æŸå¤±æŒ‡æ ‡
    avg_flow = total_flow_loss / num_batches
    avg_kl = total_kl_loss / num_batches
    return avg_flow, avg_kl


# ========== ä¸»è®­ç»ƒå‡½æ•° ==========
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_points', type=int, default=2048)
    parser.add_argument('--epochs', type=int, default=1000)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=0)
    parser.add_argument('--d_f', type=int, default=512)
    parser.add_argument('--d', type=int, default=64)
    parser.add_argument('--num_tokens', type=int, default=32)
    parser.add_argument('--kl_weight', type=float, default=1e-4)
    parser.add_argument('--data_root', type=str, default='data/ShapeNetCore.v2.PC15k')
    parser.add_argument('--patience', type=int, default=9999)
    parser.add_argument('--warmup_steps', type=int, default=4000)
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)

    # tokenizer: n = 2048, k = 32, d_f = 512, d = 64, n_head = 8
    # 5490 ä¸‡è®­ç»ƒå‚æ•°

    tokenizer = ShapeTokenizer(
        num_tokens=args.num_tokens,
        d_in=3,
        d_f=args.d_f,
        d=args.d,
        n_heads=8,
        num_frequencies=16,
        num_blocks=6
    ).to(device)

    estimator = VelocityEstimator(
        d=args.d_f, # TODO d_f è¿˜æ˜¯ d? 
        num_frequencies=16,
        n_blocks=3
    ).to(device)

    # ===== æ‰“å° Tokenizer ç»“æ„ä¸å‚æ•° =====
    print("\nğŸ“¦ ShapeTokenizer Architecture:\n")
    print(tokenizer)
    tokenizer_params = sum(p.numel() for p in tokenizer.parameters() if p.requires_grad)
    print(f"ğŸ“Š ShapeTokenizer trainable parameters: {tokenizer_params / 1e6:.2f} M")

    # ===== æ‰“å° VelocityEstimator ç»“æ„ä¸å‚æ•° =====
    print("\nğŸ“¦ VelocityEstimator Architecture:\n")
    print(estimator)
    estimator_params = sum(p.numel() for p in estimator.parameters() if p.requires_grad)
    print(f"ğŸ“Š VelocityEstimator trainable parameters: {estimator_params / 1e6:.2f} M")

    total_params = sum(p.numel() for p in tokenizer.parameters() if p.requires_grad) \
             + sum(p.numel() for p in estimator.parameters() if p.requires_grad)
    print(f"ğŸ“Š Total ShapeNet LFM trainable parameters: {total_params / 1e6:.2f} M")


    optimizer = optim.AdamW(
        list(tokenizer.parameters()) + list(estimator.parameters()),
        lr=args.lr,
        betas=(0.9, 0.98),     
        weight_decay=args.weight_decay
    )
    scheduler = LambdaLR(optimizer, lr_lambda=get_transformer_lr_schedule(args.warmup_steps, args.d_f))

    train_loader = get_dataloader(
        root=args.data_root,
        split='train',
        batch_size=args.batch_size,
        num_points=args.num_points,
        shuffle=True,
        num_workers=4
    )

    val_loader = get_dataloader(
        root=args.data_root,
        split='val',
        batch_size=args.batch_size,
        num_points=args.num_points,
        shuffle=False,
        num_workers=4
    )

    os.makedirs("checkpoints", exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join("logs", timestamp)
    os.makedirs(log_dir, exist_ok=True)

    log = {
        "config": vars(args),
        "train": []
    }
    best_val = float("inf")
    patience_counter = 0
    print(f"Training ShapeTokenizer and VelocityEstimator with config: {args}")
    for epoch in range(1, args.epochs + 1):
        print(f"Epoch {epoch}/{args.epochs}")
        train_flow, train_kl = run_one_epoch(tokenizer, estimator, train_loader, optimizer, scheduler, device, args.kl_weight, mode="train")
        val_flow, val_kl = run_one_epoch(tokenizer, estimator, val_loader, optimizer=None, scheduler=None, device=device, kl_weight=args.kl_weight, mode="val")

        print(f"[Epoch {epoch}] Train Flow: {train_flow:.6f} | Train KL: {train_kl:.6f} || Val Flow: {val_flow:.6f} | Val KL: {val_kl:.6f}")

        log["train"].append({
            "epoch": epoch,
            "train_flow": train_flow,
            "train_kl": train_kl,
            "val_flow": val_flow,
            "val_kl": val_kl
        })

        if val_flow < best_val:
            best_val = val_flow
            patience_counter = 0
            torch.save(tokenizer.state_dict(), "checkpoints/best_tokenizer.pt")
            torch.save(estimator.state_dict(), "checkpoints/best_estimator.pt")
            print("âœ… Best model updated.")
        else:
            patience_counter += 1
            print(f"â³ Early stopping patience: {patience_counter}/{args.patience}")
            if patience_counter >= args.patience:
                print("â¹ï¸ Early stopping triggered.")
                break

        log_path = os.path.join(log_dir, "train_log.json")
    with open(log_path, "w") as f:
        json.dump(log, f, indent=2)


if __name__ == '__main__':
    main()
